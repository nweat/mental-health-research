{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.plotly as plotl\n",
    "import plotly.graph_objs as go\n",
    "import plotly.tools as tls\n",
    "import urllib2\n",
    "import urllib\n",
    "import time\n",
    "import json\n",
    "import nltk\n",
    "import string\n",
    "import geocoder\n",
    "import re\n",
    "import os, csv\n",
    "import collections\n",
    "import preprocessor as p\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from wordcloud import WordCloud\n",
    "from plotly.tools import FigureFactory as FF\n",
    "from textblob import TextBlob\n",
    "from pandas.io.json import json_normalize\n",
    "from functools import partial\n",
    "#from multiprocessing import Pool\n",
    "from pattern.text.en import singularize\n",
    "from pattern.en import parse\n",
    "from pattern.en import tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from collections import Counter\n",
    "from unicodedata import normalize\n",
    "import unicodedata as ud\n",
    "from nltk import bigrams \n",
    "from geopy.geocoders import Nominatim\n",
    "plt.style.use('ggplot')  \n",
    "tls.set_credentials_file(username='nweat', api_key='BgNMfzTIbo5F2wPiSCCF')\n",
    "from unidecode import unidecode\n",
    "import sys\n",
    "default_stdout = sys.stdout\n",
    "default_stderr = sys.stderr\n",
    "reload(sys)\n",
    "sys.setdefaultencoding('utf-8')\n",
    "sys.stdout = default_stdout\n",
    "sys.stderr = default_stderr\n",
    "pd.set_option('max_colwidth', 800)\n",
    "###################################################################\n",
    "## USEFUL FOR ANALYSIS, VISUALIZATIONS\n",
    "#http://kyrandale.com/static/talks/reveal.js/index_pydata2015.html#/3\n",
    "#http://adilmoujahid.com/posts/2015/01/interactive-data-visualization-d3-dc-python-mongodb/\n",
    "#http://www.d3noob.org/2013/02/update-d3js-data-dynamically-button.html\n",
    "#textblob\n",
    "#https://www.quora.com/What-are-the-best-ways-to-do-Twitter-sentiment-analysis-in-R-with-offline-datasets\n",
    "#csvDf.to_json(path_or_buf = None, orient = 'records', date_format = 'epoch', double_precision = 10, force_ascii = True, date_unit = 'ms', default_handler = None)\n",
    "#http://machinelearningmastery.com/time-series-data-visualization-with-python/\n",
    "#http://saifmohammad.com/WebPages/NRC-Emotion-Lexicon.htm\n",
    "#https://www.analyticsvidhya.com/blog/2016/02/time-series-forecasting-codes-python/\n",
    "#https://www.digitalocean.com/community/tutorials/a-guide-to-time-series-visualization-with-python-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Lemma#PoS</th>\n",
       "      <th>AFRAID</th>\n",
       "      <th>AMUSED</th>\n",
       "      <th>ANGRY</th>\n",
       "      <th>ANNOYED</th>\n",
       "      <th>DONT_CARE</th>\n",
       "      <th>HAPPY</th>\n",
       "      <th>INSPIRED</th>\n",
       "      <th>SAD</th>\n",
       "      <th>POS</th>\n",
       "      <th>Lemma</th>\n",
       "      <th>NLTKPos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13208</th>\n",
       "      <td>friend#n</td>\n",
       "      <td>0.052252</td>\n",
       "      <td>0.151599</td>\n",
       "      <td>0.089777</td>\n",
       "      <td>0.116535</td>\n",
       "      <td>0.164726</td>\n",
       "      <td>0.102121</td>\n",
       "      <td>0.177929</td>\n",
       "      <td>0.145061</td>\n",
       "      <td>n</td>\n",
       "      <td>friend</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14876</th>\n",
       "      <td>happy#a</td>\n",
       "      <td>0.054755</td>\n",
       "      <td>0.152320</td>\n",
       "      <td>0.054510</td>\n",
       "      <td>0.106943</td>\n",
       "      <td>0.169513</td>\n",
       "      <td>0.161437</td>\n",
       "      <td>0.206614</td>\n",
       "      <td>0.093908</td>\n",
       "      <td>a</td>\n",
       "      <td>happy</td>\n",
       "      <td>JJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18161</th>\n",
       "      <td>kill#n</td>\n",
       "      <td>0.066652</td>\n",
       "      <td>0.086503</td>\n",
       "      <td>0.108772</td>\n",
       "      <td>0.080200</td>\n",
       "      <td>0.172450</td>\n",
       "      <td>0.176482</td>\n",
       "      <td>0.068441</td>\n",
       "      <td>0.240500</td>\n",
       "      <td>n</td>\n",
       "      <td>kill</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18162</th>\n",
       "      <td>kill#v</td>\n",
       "      <td>0.234204</td>\n",
       "      <td>0.058491</td>\n",
       "      <td>0.209312</td>\n",
       "      <td>0.066826</td>\n",
       "      <td>0.054523</td>\n",
       "      <td>0.058151</td>\n",
       "      <td>0.047547</td>\n",
       "      <td>0.270947</td>\n",
       "      <td>v</td>\n",
       "      <td>kill</td>\n",
       "      <td>V</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19384</th>\n",
       "      <td>lonely#a</td>\n",
       "      <td>0.035667</td>\n",
       "      <td>0.130936</td>\n",
       "      <td>0.054746</td>\n",
       "      <td>0.090958</td>\n",
       "      <td>0.074914</td>\n",
       "      <td>0.121755</td>\n",
       "      <td>0.233194</td>\n",
       "      <td>0.257829</td>\n",
       "      <td>a</td>\n",
       "      <td>lonely</td>\n",
       "      <td>JJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22150</th>\n",
       "      <td>never#r</td>\n",
       "      <td>0.077574</td>\n",
       "      <td>0.141194</td>\n",
       "      <td>0.098921</td>\n",
       "      <td>0.128621</td>\n",
       "      <td>0.125786</td>\n",
       "      <td>0.110980</td>\n",
       "      <td>0.198967</td>\n",
       "      <td>0.117956</td>\n",
       "      <td>r</td>\n",
       "      <td>never</td>\n",
       "      <td>RB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28606</th>\n",
       "      <td>sad#a</td>\n",
       "      <td>0.056234</td>\n",
       "      <td>0.123854</td>\n",
       "      <td>0.094913</td>\n",
       "      <td>0.135547</td>\n",
       "      <td>0.129764</td>\n",
       "      <td>0.065862</td>\n",
       "      <td>0.151933</td>\n",
       "      <td>0.241893</td>\n",
       "      <td>a</td>\n",
       "      <td>sad</td>\n",
       "      <td>JJ</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Lemma#PoS    AFRAID    AMUSED     ANGRY   ANNOYED  DONT_CARE     HAPPY  \\\n",
       "13208  friend#n  0.052252  0.151599  0.089777  0.116535   0.164726  0.102121   \n",
       "14876   happy#a  0.054755  0.152320  0.054510  0.106943   0.169513  0.161437   \n",
       "18161    kill#n  0.066652  0.086503  0.108772  0.080200   0.172450  0.176482   \n",
       "18162    kill#v  0.234204  0.058491  0.209312  0.066826   0.054523  0.058151   \n",
       "19384  lonely#a  0.035667  0.130936  0.054746  0.090958   0.074914  0.121755   \n",
       "22150   never#r  0.077574  0.141194  0.098921  0.128621   0.125786  0.110980   \n",
       "28606     sad#a  0.056234  0.123854  0.094913  0.135547   0.129764  0.065862   \n",
       "\n",
       "       INSPIRED       SAD POS   Lemma NLTKPos  \n",
       "13208  0.177929  0.145061   n  friend      NN  \n",
       "14876  0.206614  0.093908   a   happy      JJ  \n",
       "18161  0.068441  0.240500   n    kill      NN  \n",
       "18162  0.047547  0.270947   v    kill       V  \n",
       "19384  0.233194  0.257829   a  lonely      JJ  \n",
       "22150  0.198967  0.117956   r   never      RB  \n",
       "28606  0.151933  0.241893   a     sad      JJ  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##################################################################################\n",
    "# LOAD VARIABLES\n",
    "###################################################################################\n",
    "mood_lexicon_output = 'final_data/mood_lexicon.csv'\n",
    "CSV_labeledMissingGeo_output = 'final_data/labelMIssingGeo.csv' #output from multi_processing_file\n",
    "CSV_preprocessing_output = 'final_data/labelMIssingGeo_preprocessed.csv'\n",
    "CSV_sentiment140_output = 'final_data/labelMIssingGeo_preprocessed_sentiment.csv'\n",
    "CSV_emotion_output = 'final_data/labelMIssingGeo_preprocessed_sentiment_emotion.csv'\n",
    "CSV_keywords_output = 'final_data/labelMIssingGeo_preprocessed_sentiment_emotion_keywords.csv'\n",
    "tweets = pd.read_table(CSV_labeledMissingGeo_output, sep=',') #initial data loading\n",
    "\n",
    "\n",
    "##################################################################################\n",
    "# HELPER FUNCTIONS\n",
    "###################################################################################\n",
    "\n",
    "# input - df: a Dataframe, chunkSize: the chunk size\n",
    "# output - a list of DataFrame\n",
    "# purpose - splits the DataFrame into smaller of max size chunkSize (last is smaller)\n",
    "# http://stackoverflow.com/questions/17315737/split-a-large-pandas-dataframe\n",
    "def splitDataFrameIntoSmaller(df, chunkSize = 10000): \n",
    "    listOfDf = list()\n",
    "    numberChunks = len(df) // chunkSize + 1\n",
    "    for i in range(numberChunks):\n",
    "        listOfDf.append(df[i*chunkSize:(i+1)*chunkSize])\n",
    "    return listOfDf\n",
    "\n",
    "##################################################################################\n",
    "# Lable emojis with their emotion *NOT PRIORITY\n",
    "##################################################################################\n",
    "EMOJI_LIST = pd.read_csv('emoji_table.csv', encoding='utf-8')\n",
    "def unicodeEscapeEmoji(row):\n",
    "    row['unicode_escape'] = row['emoji'].encode('unicode_escape')\n",
    "    return row\n",
    "EMOJI_LIST = EMOJI_LIST.apply(unicodeEscapeEmoji, axis = 1)\n",
    "EMOJI_LIST.head(4)\n",
    "\n",
    "\n",
    "##################################################################################\n",
    "# MOOD LEXICON PREPARATION (DEPECHE MOOD LEXICON: AFRAID, AMUSED, ANGRY, ANNOYED, DONT CARE, HAPPY, INSPIRED, SAD)\n",
    "# DESCRIPTION: \n",
    "# Extract Lemma, label NTLK POS\n",
    "# REFERENCES: http://www.nltk.org/book/ch05.html\n",
    "###################################################################################\n",
    "mood_lexicon = pd.read_table('DepecheMood_V1.0/DepecheMood_normfreq.txt')\n",
    "\n",
    "def extractPOS(lemmaPos):\n",
    "    return lemmaPos.split('#')[1]\n",
    "\n",
    "def extractLemma(lemmaPos):\n",
    "    return lemmaPos.split('#')[0]\n",
    "\n",
    "def labelNLTKPos(pos):\n",
    "    if pos == 'n':\n",
    "        return 'NN'\n",
    "    elif pos == 'a':\n",
    "        return 'JJ'\n",
    "    elif pos == 'r':\n",
    "        return 'RB'\n",
    "    elif pos == 'v':\n",
    "        return 'V'\n",
    "       \n",
    "mood_lexicon['POS'] = mood_lexicon['Lemma#PoS'].apply(extractPOS)\n",
    "mood_lexicon['Lemma'] = mood_lexicon['Lemma#PoS'].apply(extractLemma)\n",
    "mood_lexicon['NLTKPos'] = mood_lexicon['POS'].apply(labelNLTKPos)\n",
    "mood_lexicon.to_csv(mood_lexicon_output, index = False)\n",
    "#mood_lexicon['Max_Weight'] = mood_lexicon[['AFRAID','AMUSED','ANGRY','ANNOYED','DONT_CARE','HAPPY','INSPIRED','SAD']].max(axis=1)\n",
    "mood_lexicon[mood_lexicon['Lemma'].isin(['friend','sad','lonely', 'kill', 'happy','never'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df1\n",
      "df2\n",
      "df3\n",
      "df4\n",
      "df5\n",
      "df6\n",
      "df7\n",
      "df8\n",
      "df9\n",
      "df10\n",
      "df11\n",
      "df12\n",
      "df13\n",
      "df14\n",
      "df15\n",
      "df16\n",
      "df17\n",
      "df18\n",
      "df19\n",
      "df20\n",
      "df21\n",
      "df22\n",
      "df23\n",
      "df24\n",
      "df25\n",
      "df26\n",
      "df27\n",
      "df28\n",
      "df29\n",
      "df30\n",
      "df31\n",
      "df32\n",
      "df33\n",
      "df34\n",
      "df35\n",
      "df36\n",
      "df37\n",
      "df38\n",
      "df39\n",
      "df40\n",
      "df41\n",
      "df42\n",
      "df43\n",
      "df44\n",
      "DONE\n"
     ]
    }
   ],
   "source": [
    "##################################################################################\n",
    "# PREPROCESS EVERY TWEET\n",
    "# DESCRIPTION:\n",
    "# Remove URL, mentions, RT word, \n",
    "# REFERENCES:\n",
    "# http://www.clips.ua.ac.be/pages/pattern-en#ngram\n",
    "# POSSIBLE FIXES:\n",
    "# Spelling errors\n",
    "# Fix stemming e.g. jesu should be jesus\n",
    "# Replace emojis and smileys with word emotion\n",
    "# To speed up: http://stackoverflow.com/questions/26784164/pandas-multiprocessing-apply\n",
    "# http://iacs-courses.seas.harvard.edu/courses/iacs_projects/sentimentanalysisforfinance.wordpress.com/data-preprocessing/index.html\n",
    "# TO ADD IF HAVE TIME:\n",
    "# ADDITIONS IF HAVE TIME: punctuations??, replace emoji with emotion name, smileys, remove retweets, \n",
    "###################################################################################\n",
    "STOP = set(stopwords.words('english'))\n",
    "p.set_options(p.OPT.NUMBER) #p.OPT.SMILEY, p.OPT.EMOJI, \n",
    "preprocessTweets = splitDataFrameIntoSmaller(tweets, 3500)\n",
    "frames = []\n",
    "dfID = 0\n",
    "\n",
    "def preprocessTweet(row):\n",
    "    # must do this preprocessing step: lower text, remove mentions,numbers, RT, URLS, # sign but leave hashtag text, stemming\n",
    "    text = row['tweetText'].lower()\n",
    "    text = text.encode('utf-8')\n",
    "    text = re.compile(r\"(?:\\@|https?\\://)\\S+\").sub('', re.compile('rt @').sub('@', text).strip())\n",
    "    text = re.sub(\"[#]\", \"\", text).strip()\n",
    "    text = p.clean(text)\n",
    "     \n",
    "    #text = unidecode(text.replace('\"',''))\n",
    "    tokenized = text.split() #word_tokenize(text)\n",
    "    tokenized = [singularize(plural) for plural in tokenized] #stemmer\n",
    "    stop_words_removed = [w for w in tokenized if not w in STOP]\n",
    "    #tokenized_postag = nltk.pos_tag(tokenized)\n",
    "    nouns = [word for word, pos in tag(text) if pos.startswith(\"N\") == True]\n",
    "    verbs = [word for word, pos in tag(text) if pos.startswith(\"V\") == True]\n",
    "    adverb = [word for word, pos in tag(text) if pos.startswith(\"R\") == True]\n",
    "    adjectives = [word for word, pos in tag(text) if pos.startswith(\"J\") == True]\n",
    "    row['stopWordsRemoved'] = stop_words_removed\n",
    "    row['withStopWords'] = tokenized\n",
    "    row['adjectives'] = adjectives\n",
    "    row['verbs'] = verbs\n",
    "    row['adverb'] = adverb\n",
    "    row['nouns'] = nouns\n",
    "    return row\n",
    "\n",
    "for i in preprocessTweets:\n",
    "    dfID += 1\n",
    "    dfName = 'df' + str(dfID)\n",
    "    print dfName\n",
    "    i = i.apply(preprocessTweet, axis = 1)\n",
    "    frames.append(i)\n",
    "\n",
    "preprocessedTweets = pd.concat(frames)\n",
    "preprocessedTweets.to_csv(CSV_preprocessing_output, index = False)\n",
    "print 'DONE'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "10000\n",
      "10000\n",
      "10000\n",
      "10000\n",
      "10000\n",
      "10000\n",
      "10000\n",
      "10000\n",
      "10000\n",
      "10000\n",
      "10000\n",
      "10000\n",
      "10000\n",
      "10000\n",
      "3884\n",
      "153884\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>username</th>\n",
       "      <th>tweetCreated</th>\n",
       "      <th>tweetLang</th>\n",
       "      <th>tweetText</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>mentions</th>\n",
       "      <th>favorites</th>\n",
       "      <th>retweet</th>\n",
       "      <th>tweetLat</th>\n",
       "      <th>tweetLong</th>\n",
       "      <th>...</th>\n",
       "      <th>stopWordsRemoved</th>\n",
       "      <th>withStopWords</th>\n",
       "      <th>adjectives</th>\n",
       "      <th>verbs</th>\n",
       "      <th>adverb</th>\n",
       "      <th>nouns</th>\n",
       "      <th>meta.headline</th>\n",
       "      <th>meta.language</th>\n",
       "      <th>meta.topic</th>\n",
       "      <th>polarity</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>badkidvictoria</td>\n",
       "      <td>2017-02-16 12:56:11</td>\n",
       "      <td>en</td>\n",
       "      <td>@bbceastenders ugh, why?</td>\n",
       "      <td>NaN</td>\n",
       "      <td>@bbceastenders</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>53.49368</td>\n",
       "      <td>-2.186851</td>\n",
       "      <td>...</td>\n",
       "      <td>[ugh,, why?]</td>\n",
       "      <td>[ugh,, why?]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>en</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          username         tweetCreated tweetLang                 tweetText  \\\n",
       "id                                                                            \n",
       "0   badkidvictoria  2017-02-16 12:56:11        en  @bbceastenders ugh, why?   \n",
       "\n",
       "   hashtags        mentions  favorites  retweet  tweetLat  tweetLong   ...     \\\n",
       "id                                                                     ...      \n",
       "0       NaN  @bbceastenders          0        0  53.49368  -2.186851   ...      \n",
       "\n",
       "   stopWordsRemoved withStopWords adjectives verbs adverb nouns meta.headline  \\\n",
       "id                                                                              \n",
       "0      [ugh,, why?]  [ugh,, why?]         []    []     []    []           NaN   \n",
       "\n",
       "   meta.language meta.topic polarity  \n",
       "id                                    \n",
       "0             en        NaN        0  \n",
       "\n",
       "[1 rows x 22 columns]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##################################################################################\n",
    "# SENTIMENT 140 TWEET LABELLING\n",
    "# SAMPLE INPUT: '{\"data\": [{\"text\": \"But I also pray for peace and healing.\"},{\"text\": \"I hate Titanic.\"}]}'\n",
    "# SAMPLE OUTPUT: '{\"data\":[{\"text\":\"I love Titanic.\",\"polarity\":4}]}'\n",
    "# map back to original structure along with newly added sentiment label\n",
    "# REFERENCES: https://www.webstreaming.com.ar/articles/using-sentiment-analysis-and-python-to-evaluate-image-in-twitter-for-the-most-important-argentinian-candidates/\n",
    "###################################################################################\n",
    "dfID = 0\n",
    "tweets = []\n",
    "frames = []\n",
    "\n",
    "try:\n",
    "    preprocessedTweets\n",
    "except NameError:\n",
    "    preprocessedTweets = pd.read_csv(CSV_preprocessing_output)\n",
    "\n",
    "labelSentiment = splitDataFrameIntoSmaller(preprocessedTweets, 10000)\n",
    "sentimentLabelledTweets = pd.DataFrame()\n",
    "\n",
    "def formatData(row):\n",
    "    text = \" \".join(row.withStopWords).strip()\n",
    "    tweets.append({'id': row.name,\n",
    "                   'text': unidecode(text) # CHANGE TO USE FIELD 'withStopWords'\n",
    "                  })\n",
    "\n",
    "for i in labelSentiment:\n",
    "    dfID += 1\n",
    "    dfName = 'df' + str(dfID)\n",
    "    i.apply(formatData, axis = 1)\n",
    "    toSend = {\"data\" : tweets}\n",
    "    req = urllib2.Request(\"http://www.sentiment140.com/api/bulkClassifyJson\")\n",
    "    req.add_header('Content-Type', 'application/json')\n",
    "    response = urllib2.urlopen(req, str(toSend))\n",
    "    json_response = json.loads(response.read())\n",
    "    dfName = json_normalize(json_response['data'])\n",
    "    dfName = dfName.set_index(['id'])\n",
    "    frames.append(dfName)\n",
    "    tweets = []\n",
    "    print len(dfName)\n",
    "    \n",
    "combined_frames = pd.concat(frames) \n",
    "sentimentLabelledTweets = pd.concat([preprocessedTweets, combined_frames], axis=1, join='inner') \n",
    "sentimentLabelledTweets.drop('userid', axis=1, inplace=True)\n",
    "sentimentLabelledTweets.drop('text', axis=1, inplace=True)\n",
    "sentimentLabelledTweets.to_csv(CSV_sentiment140_output, index = False, encoding = 'utf-8')\n",
    "print len(sentimentLabelledTweets) \n",
    "sentimentLabelledTweets.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "(\"'str' object has no attribute 'values'\", u'occurred at index 0')",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-dcc2f9a46619>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0mframes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m \u001b[0msentimentLabelledTweets_emotion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msentimentLabelledTweets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcalcMoodStates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0msentimentLabelledTweets_emotion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'username'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'tweetCreated'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'tweetText'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'polarity'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/dracula/.local/lib/python2.7/site-packages/pandas/core/frame.pyc\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, axis, broadcast, raw, reduce, args, **kwds)\u001b[0m\n\u001b[1;32m   4150\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4151\u001b[0m                         \u001b[0mreduce\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4152\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply_standard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4153\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4154\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply_broadcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/dracula/.local/lib/python2.7/site-packages/pandas/core/frame.pyc\u001b[0m in \u001b[0;36m_apply_standard\u001b[0;34m(self, func, axis, ignore_failures, reduce)\u001b[0m\n\u001b[1;32m   4246\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4247\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseries_gen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4248\u001b[0;31m                     \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4249\u001b[0m                     \u001b[0mkeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4250\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-18-dcc2f9a46619>\u001b[0m in \u001b[0;36mcalcMoodStates\u001b[0;34m(sentimentLabelledTweets)\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;31m#AFRAID_VALUES = []\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0mFOUND_WORDS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentimentLabelledTweets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'stopWordsRemoved'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m         \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSAD_VALUES\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0msentimentLabelledTweets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'SAD'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSAD_VALUES\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: (\"'str' object has no attribute 'values'\", u'occurred at index 0')"
     ]
    }
   ],
   "source": [
    "##################################################################################\n",
    "# CALCULATE AND LABEL MOOD STATES OF EACH PATIENTS TWEETS BASED ON DEPECHE MOOD LEXICON\n",
    "# DESCRIPTION:\n",
    "# For each tweet, tokenize, group all scores for each mood where weighting is over 15%\n",
    "# FIXES:\n",
    "# Select which tense of the word based on POS tag to choose bcas the lexicon has multiple e.g. will#v will#n, currently choosing verb\n",
    "# REFERENCES:\n",
    "# http://stackoverflow.com/questions/30787901/how-to-get-a-value-from-a-pandas-dataframe-and-not-the-index-and-object-type\n",
    "###################################################################################\n",
    "dfID = 0\n",
    "frames = []\n",
    "threshold = 0.15 # Consider emotions weighting over 15% for every word\n",
    "\n",
    "try:\n",
    "    sentimentLabelledTweets = pd.read_csv(CSV_sentiment140_output)\n",
    "except NameError:\n",
    "    print 'test'\n",
    "    #sentimentLabelledTweets = pd.read_csv('final_data/test.csv')\n",
    "    \n",
    "labelEmotion = splitDataFrameIntoSmaller(sentimentLabelledTweets, 2000)\n",
    "mood_lexicon = pd.read_csv(mood_lexicon_output)\n",
    "\n",
    "def calcMoodStates(sentimentLabelledTweets):\n",
    "    SAD_VALUES = []\n",
    "    #INSPIRED_VALUES = []\n",
    "    #HAPPY_VALUES = []\n",
    "    #DONT_CARE_VALUES = []\n",
    "    #ANNOYED_VALUES = []\n",
    "    #ANGRY_VALUES = []\n",
    "    #AMUSED_VALUES = []\n",
    "    #AFRAID_VALUES = []\n",
    "    FOUND_WORDS = 0\n",
    "    for token in sentimentLabelledTweets['stopWordsRemoved']:\n",
    "        print 'nothing'\n",
    "        #np.append(SAD_VALUES, token)\n",
    "    sentimentLabelledTweets['SAD'] = 'test'\n",
    "    #sentimentLabelledTweets['INSPIRED'] = np.mean(INSPIRED_VALUES).round(2)\n",
    "    #sentimentLabelledTweets['HAPPY'] = np.mean(HAPPY_VALUES).round(2)\n",
    "    #sentimentLabelledTweets['DONT_CARE'] = np.mean(DONT_CARE_VALUES).round(2)\n",
    "    #sentimentLabelledTweets['ANNOYED'] = np.mean(ANNOYED_VALUES).round(2)\n",
    "    #sentimentLabelledTweets['ANGRY'] = np.mean(ANGRY_VALUES).round(2)\n",
    "    #sentimentLabelledTweets['AMUSED'] = np.mean(AMUSED_VALUES).round(2)\n",
    "    #sentimentLabelledTweets['AFRAID'] = np.mean(AFRAID_VALUES).round(2)\n",
    "    #sentimentLabelledTweets['FOUND_WORDS'] = FOUND_WORDS\n",
    "    return sentimentLabelledTweets\n",
    "\n",
    "for i in labelEmotion:\n",
    "    dfID += 1\n",
    "    dfName = 'df' + str(dfID)\n",
    "    #print dfName\n",
    "    #i = i.apply(calcMoodStates, axis = 1)\n",
    "    frames.append(i)\n",
    "    \n",
    "sentimentLabelledTweets_emotion = sentimentLabelledTweets.apply(calcMoodStates, axis = 1)\n",
    "sentimentLabelledTweets_emotion[['username','tweetCreated','tweetText','polarity']]\n",
    "\n",
    "#emotionLabelled = pd.concat(frames)\n",
    "#emotionLabelled.to_csv(CSV_emotion_output, index = False)\n",
    "#emotionLabelled[['username','stopWordsRemoved','tweetText','polarity']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>username</th>\n",
       "      <th>tweetCreated</th>\n",
       "      <th>tweetText</th>\n",
       "      <th>polarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Demolarsarah</td>\n",
       "      <td>12/28/2016 18:41</td>\n",
       "      <td>iraq car bombings kill people, wound more than</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Demolarsarah</td>\n",
       "      <td>12/28/2016 5:38</td>\n",
       "      <td>i just feel so miserable right now ðŸ˜”</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Demolarsarah</td>\n",
       "      <td>12/28/2016 5:38</td>\n",
       "      <td>i just feel so miserable right now ðŸ˜”</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Demolarsarah</td>\n",
       "      <td>12/28/2016 5:40</td>\n",
       "      <td>i just feel so miserable right now ðŸ˜”</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Demolarsarah</td>\n",
       "      <td>12/28/2016 5:48</td>\n",
       "      <td>i just feel so miserable right now ðŸ˜”</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Demolarsarah</td>\n",
       "      <td>12/24/2016 5:50</td>\n",
       "      <td>from the #syrian people to the greatest human in the world thank you we wish if there's many people #standwitâ€¦</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Demolarsarah</td>\n",
       "      <td>12/28/2016 3:32</td>\n",
       "      <td>no one asks to have bipolar, it is a really shitty disease, so is cancer, should cancer be a way we describe a person?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Nikki</td>\n",
       "      <td>12/28/2016 5:48</td>\n",
       "      <td>i just feel so happy right now ðŸ˜”</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Nikki</td>\n",
       "      <td>12/29/2016 5:48</td>\n",
       "      <td>i just feel so miserable right now ðŸ˜”</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Nikki</td>\n",
       "      <td>12/30/2016 5:48</td>\n",
       "      <td>i just feel so happy right now ðŸ˜”</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       username      tweetCreated  \\\n",
       "0  Demolarsarah  12/28/2016 18:41   \n",
       "1  Demolarsarah   12/28/2016 5:38   \n",
       "2  Demolarsarah   12/28/2016 5:38   \n",
       "3  Demolarsarah   12/28/2016 5:40   \n",
       "4  Demolarsarah   12/28/2016 5:48   \n",
       "5  Demolarsarah   12/24/2016 5:50   \n",
       "6  Demolarsarah   12/28/2016 3:32   \n",
       "7         Nikki   12/28/2016 5:48   \n",
       "8         Nikki   12/29/2016 5:48   \n",
       "9         Nikki   12/30/2016 5:48   \n",
       "\n",
       "                                                                                                                tweetText  \\\n",
       "0                                                                          iraq car bombings kill people, wound more than   \n",
       "1                                                                                 i just feel so miserable right now ðŸ˜”   \n",
       "2                                                                                 i just feel so miserable right now ðŸ˜”   \n",
       "3                                                                                 i just feel so miserable right now ðŸ˜”   \n",
       "4                                                                                 i just feel so miserable right now ðŸ˜”   \n",
       "5        from the #syrian people to the greatest human in the world thank you we wish if there's many people #standwitâ€¦   \n",
       "6  no one asks to have bipolar, it is a really shitty disease, so is cancer, should cancer be a way we describe a person?   \n",
       "7                                                                                     i just feel so happy right now ðŸ˜”   \n",
       "8                                                                                 i just feel so miserable right now ðŸ˜”   \n",
       "9                                                                                     i just feel so happy right now ðŸ˜”   \n",
       "\n",
       "   polarity  \n",
       "0         2  \n",
       "1         0  \n",
       "2         0  \n",
       "3         0  \n",
       "4         0  \n",
       "5         2  \n",
       "6         0  \n",
       "7         2  \n",
       "8         0  \n",
       "9         2  "
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##################################################################################\n",
    "# KEYWORDS LABELLING TO ATTEMPT TO CAPTURE SPECIFIC SYMPTOMS INCLUDING:\n",
    "# suicidal thoughts (depressive mood)\n",
    "# physical symptoms e.g. weight gain, loss of interest, cyrying, headaches (depressive mood)\n",
    "# alcohol use (start of mania (hypomania))\n",
    "# high sex drive (start of mania (hypomania))\n",
    "# spending sprees (start of mania (hypomania))\n",
    "# religious inspirations (start of mania (hypomania))\n",
    "\n",
    "# DESCRIPTION:\n",
    "# Find occurences of defined keywords in tweets and label\n",
    "###################################################################################\n",
    "\n",
    "\n",
    "# chosen based on tf-idf results extracted from sample of bipolar patients tweets\n",
    "# MOST COMMON 200 WORDS SELECTED FOR EACH USER AND BASED ON THAT DID TF-IDF\n",
    "idfhashtags = [\n",
    "    'bipolar',\n",
    "    'depress',\n",
    "    'depressed',\n",
    "    'mentalhealth',\n",
    "    'worldmentalhealthday',\n",
    "    'mentalhealthawareness',\n",
    "    'anxiety',\n",
    "    'bipolardisorder',\n",
    "    'mentallillness'\n",
    "]\n",
    "\n",
    "## SUICIDE KEYWORDS\n",
    "suicide_words = [\n",
    "    'suicide',\n",
    "    'suicidal',\n",
    "    'never wake up',\n",
    "    'kill myself',\n",
    "    'end my life',\n",
    "    'cant go on',\n",
    "    \"can't go on\",\n",
    "    'not worth living',\n",
    "    'ready to jump',\n",
    "    'sleep forever',\n",
    "    'want to die',\n",
    "    'be dead',\n",
    "    'better off without me',\n",
    "    'better off dead',\n",
    "    'tired of living',\n",
    "    \"don't want to be here\",\n",
    "    'dont want to be here',\n",
    "    'die alone',\n",
    "    'die',\n",
    "    'dead',\n",
    "    'sleep forever'\n",
    "]\n",
    "\n",
    "\n",
    "## BODY SYMPTOMS KEYWORDS\n",
    "phyiscal_symptoms = [\n",
    "    'gain weight',\n",
    "    'lose weight',\n",
    "    'fat',\n",
    "    'skinny',\n",
    "    'constipated',\n",
    "    'crying',\n",
    "    'headache',\n",
    "    'pain',\n",
    "    'painful',\n",
    "    'no sex life',\n",
    "    'sexless',\n",
    "    'not interested',\n",
    "    'lose interest',\n",
    "    'tired',\n",
    "    \n",
    "]\n",
    "\n",
    "\n",
    "## MANIC SYMPTOMS\n",
    "# ALCOHOL ABUSE KEYWORDS\n",
    "# https://myvocabulary.com/word-list/alcohol-abuse-vocabulary/\n",
    "alcohol_abuse = [\n",
    "    'addict',\n",
    "    'addiction',\n",
    "    'alcoholic',\n",
    "    'alcohol',\n",
    "    'bar',\n",
    "    'drug',\n",
    "    'high',\n",
    "    'drunk',\n",
    "    'party',\n",
    "    'drinking',\n",
    "    'drink',\n",
    "    'smoke',\n",
    "    'liquor',\n",
    "    'pills'\n",
    "]\n",
    "\n",
    "# RELIGIOUS INPIRATION KEYWORDS\n",
    "religious = [\n",
    "    'god',\n",
    "    'pray',\n",
    "    'inspire',\n",
    "    'inspired',\n",
    "    'hope',\n",
    "    'jesus',\n",
    "    'love'\n",
    "]\n",
    "\n",
    "\n",
    "# MEDICATION MENTIONS\n",
    "#https://www.verywell.com/bipolar-medication-alcohol-interactions-379638\n",
    "medications = [\n",
    "    'lithium',\n",
    "    'lamotrigine',\n",
    "    'lamicta',\n",
    "    'valproic',\n",
    "    'antidepressant',\n",
    "    'latuda',\n",
    "    'benzodiazepines',\n",
    "    'recover',\n",
    "    'recovery'\n",
    "]\n",
    "\n",
    "def criteriaMatch(word, text):\n",
    "    match = False\n",
    "    match_criteria1 = re.search(word, text)\n",
    "    if match_criteria1 != None: \n",
    "        match = True\n",
    "    return match\n",
    "    \n",
    "def labelKeywordChecks(bipolar_tweets_senti140_emotion):\n",
    "    hashtags_matched = 0\n",
    "    suicide_words_matched = 0\n",
    "    phyiscal_symptoms_matched = 0\n",
    "    alcohol_abuse_words_matched = 0\n",
    "    religious_words_matched = 0\n",
    "    medications_matched = 0\n",
    "    \n",
    "    total_words = len(bipolar_tweets_senti140_emotion['tweetText'].split())\n",
    "    tweet = str(bipolar_tweets_senti140_emotion['tweetText'])\n",
    "    \n",
    "    for h in idfhashtags:\n",
    "        if criteriaMatch(h, tweet): hashtags_matched += 1\n",
    "    for s in suicide_words:\n",
    "        if criteriaMatch(s, tweet): suicide_words_matched += 1 \n",
    "    for p in phyiscal_symptoms:\n",
    "        if criteriaMatch(p, tweet): phyiscal_symptoms_matched += 1\n",
    "    for a in alcohol_abuse:\n",
    "        if criteriaMatch(a, tweet): alcohol_abuse_words_matched += 1\n",
    "    for r in religious:\n",
    "        if criteriaMatch(r, tweet): religious_words_matched += 1\n",
    "    for m in medications:\n",
    "        if criteriaMatch(m, tweet): medications_matched += 1 \n",
    "            \n",
    "    bipolar_tweets_senti140_emotion['idfhashtags_matched'] = hashtags_matched\n",
    "    bipolar_tweets_senti140_emotion['suic_words_matched'] = suicide_words_matched\n",
    "    bipolar_tweets_senti140_emotion['phyiscal_symptoms_matched'] = phyiscal_symptoms_matched\n",
    "    bipolar_tweets_senti140_emotion['alcohol_abuse_words_matched'] = alcohol_abuse_words_matched\n",
    "    bipolar_tweets_senti140_emotion['religious_words_matched'] = religious_words_matched\n",
    "    bipolar_tweets_senti140_emotion['medications_matched'] = medications_matched\n",
    "    return bipolar_tweets_senti140_emotion\n",
    "    \n",
    "bipolar_tweets_senti140_emotion_keywrds = bipolar_tweets_senti140_emotion.apply(labelKeywordChecks, axis = 1)\n",
    "#bipolar_tweets_senti140_emotion_keywrds[['username','tweetText','idfhashtags_matched','suic_words_matched']]\n",
    "bipolar_tweets_senti140_emotion_keywrds[['username','tweetCreated','tweetText','polarity']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>username</th>\n",
       "      <th>tweetCreated</th>\n",
       "      <th>tweetText</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>mentions</th>\n",
       "      <th>favorites</th>\n",
       "      <th>retweet</th>\n",
       "      <th>meta.language</th>\n",
       "      <th>polarity</th>\n",
       "      <th>tokenized</th>\n",
       "      <th>...</th>\n",
       "      <th>ANGRY</th>\n",
       "      <th>AMUSED</th>\n",
       "      <th>AFRAID</th>\n",
       "      <th>FOUND_WORDS</th>\n",
       "      <th>idfhashtags_matched</th>\n",
       "      <th>suic_words_matched</th>\n",
       "      <th>phyiscal_symptoms_matched</th>\n",
       "      <th>alcohol_abuse_words_matched</th>\n",
       "      <th>religious_words_matched</th>\n",
       "      <th>medications_matched</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Demolarsarah</td>\n",
       "      <td>12/28/2016 18:41</td>\n",
       "      <td>iraq car bombings kill people, wound more than</td>\n",
       "      <td>#exodus</td>\n",
       "      <td>@frontlinepbs</td>\n",
       "      <td>0</td>\n",
       "      <td>94</td>\n",
       "      <td>en</td>\n",
       "      <td>2</td>\n",
       "      <td>[iraq, car, bombing, kill, person, ,, wound, more, than]</td>\n",
       "      <td>...</td>\n",
       "      <td>0.23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.25</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Demolarsarah</td>\n",
       "      <td>12/28/2016 5:38</td>\n",
       "      <td>i just feel so miserable right now ðŸ˜”</td>\n",
       "      <td>NaN</td>\n",
       "      <td>@IAmRezaF</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>[i, just, feel, so, miserable, right, now, ðŸ˜”]</td>\n",
       "      <td>...</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.18</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Demolarsarah</td>\n",
       "      <td>12/28/2016 5:38</td>\n",
       "      <td>i just feel so miserable right now ðŸ˜”</td>\n",
       "      <td>NaN</td>\n",
       "      <td>@IAmRezaF</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>[i, just, feel, so, miserable, right, now, ðŸ˜”]</td>\n",
       "      <td>...</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.18</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Demolarsarah</td>\n",
       "      <td>12/28/2016 5:40</td>\n",
       "      <td>i just feel so miserable right now ðŸ˜”</td>\n",
       "      <td>NaN</td>\n",
       "      <td>@IAmRezaF</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>[i, just, feel, so, miserable, right, now, ðŸ˜”]</td>\n",
       "      <td>...</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.18</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Demolarsarah</td>\n",
       "      <td>12/28/2016 5:48</td>\n",
       "      <td>i just feel so miserable right now ðŸ˜”</td>\n",
       "      <td>NaN</td>\n",
       "      <td>@IAmRezaF</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>[i, just, feel, so, miserable, right, now, ðŸ˜”]</td>\n",
       "      <td>...</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.18</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       username      tweetCreated  \\\n",
       "0  Demolarsarah  12/28/2016 18:41   \n",
       "1  Demolarsarah   12/28/2016 5:38   \n",
       "2  Demolarsarah   12/28/2016 5:38   \n",
       "3  Demolarsarah   12/28/2016 5:40   \n",
       "4  Demolarsarah   12/28/2016 5:48   \n",
       "\n",
       "                                        tweetText hashtags       mentions  \\\n",
       "0  iraq car bombings kill people, wound more than  #exodus  @frontlinepbs   \n",
       "1         i just feel so miserable right now ðŸ˜”      NaN      @IAmRezaF   \n",
       "2         i just feel so miserable right now ðŸ˜”      NaN      @IAmRezaF   \n",
       "3         i just feel so miserable right now ðŸ˜”      NaN      @IAmRezaF   \n",
       "4         i just feel so miserable right now ðŸ˜”      NaN      @IAmRezaF   \n",
       "\n",
       "   favorites  retweet meta.language  polarity  \\\n",
       "0          0       94            en         2   \n",
       "1          0        0            en         0   \n",
       "2          0        0            en         0   \n",
       "3          0        0            en         0   \n",
       "4          0        0            en         0   \n",
       "\n",
       "                                                  tokenized  \\\n",
       "0  [iraq, car, bombing, kill, person, ,, wound, more, than]   \n",
       "1          [i, just, feel, so, miserable, right, now, ðŸ˜”]   \n",
       "2          [i, just, feel, so, miserable, right, now, ðŸ˜”]   \n",
       "3          [i, just, feel, so, miserable, right, now, ðŸ˜”]   \n",
       "4          [i, just, feel, so, miserable, right, now, ðŸ˜”]   \n",
       "\n",
       "          ...           ANGRY  AMUSED  AFRAID  FOUND_WORDS  \\\n",
       "0         ...            0.23     NaN    0.25            7   \n",
       "1         ...            0.18    0.17    0.18            7   \n",
       "2         ...            0.18    0.17    0.18            7   \n",
       "3         ...            0.18    0.17    0.18            7   \n",
       "4         ...            0.18    0.17    0.18            7   \n",
       "\n",
       "   idfhashtags_matched  suic_words_matched  phyiscal_symptoms_matched  \\\n",
       "0                    0                   0                          0   \n",
       "1                    0                   0                          0   \n",
       "2                    0                   0                          0   \n",
       "3                    0                   0                          0   \n",
       "4                    0                   0                          0   \n",
       "\n",
       "   alcohol_abuse_words_matched  religious_words_matched  medications_matched  \n",
       "0                            0                        0                    0  \n",
       "1                            0                        0                    0  \n",
       "2                            0                        0                    0  \n",
       "3                            0                        0                    0  \n",
       "4                            0                        0                    0  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bipolar_tweets_senti140_emotion_keywrds.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "username      tweetCreated       \n",
       "Demolarsarah  2016-12-24 05:00:00    False\n",
       "              2016-12-24 06:00:00       []\n",
       "              2016-12-24 07:00:00       []\n",
       "              2016-12-24 08:00:00       []\n",
       "              2016-12-24 09:00:00       []\n",
       "              2016-12-24 10:00:00       []\n",
       "              2016-12-24 11:00:00       []\n",
       "              2016-12-24 12:00:00       []\n",
       "              2016-12-24 13:00:00       []\n",
       "              2016-12-24 14:00:00       []\n",
       "              2016-12-24 15:00:00       []\n",
       "              2016-12-24 16:00:00       []\n",
       "              2016-12-24 17:00:00       []\n",
       "              2016-12-24 18:00:00       []\n",
       "              2016-12-24 19:00:00       []\n",
       "              2016-12-24 20:00:00       []\n",
       "              2016-12-24 21:00:00       []\n",
       "              2016-12-24 22:00:00       []\n",
       "              2016-12-24 23:00:00       []\n",
       "              2016-12-25 00:00:00       []\n",
       "              2016-12-25 01:00:00       []\n",
       "              2016-12-25 02:00:00       []\n",
       "              2016-12-25 03:00:00       []\n",
       "              2016-12-25 04:00:00       []\n",
       "              2016-12-25 05:00:00       []\n",
       "              2016-12-25 06:00:00       []\n",
       "              2016-12-25 07:00:00       []\n",
       "              2016-12-25 08:00:00       []\n",
       "              2016-12-25 09:00:00       []\n",
       "              2016-12-25 10:00:00       []\n",
       "                                     ...  \n",
       "Nikki         2016-12-29 00:00:00       []\n",
       "              2016-12-29 01:00:00       []\n",
       "              2016-12-29 02:00:00       []\n",
       "              2016-12-29 03:00:00       []\n",
       "              2016-12-29 04:00:00       []\n",
       "              2016-12-29 05:00:00     True\n",
       "              2016-12-29 06:00:00       []\n",
       "              2016-12-29 07:00:00       []\n",
       "              2016-12-29 08:00:00       []\n",
       "              2016-12-29 09:00:00       []\n",
       "              2016-12-29 10:00:00       []\n",
       "              2016-12-29 11:00:00       []\n",
       "              2016-12-29 12:00:00       []\n",
       "              2016-12-29 13:00:00       []\n",
       "              2016-12-29 14:00:00       []\n",
       "              2016-12-29 15:00:00       []\n",
       "              2016-12-29 16:00:00       []\n",
       "              2016-12-29 17:00:00       []\n",
       "              2016-12-29 18:00:00       []\n",
       "              2016-12-29 19:00:00       []\n",
       "              2016-12-29 20:00:00       []\n",
       "              2016-12-29 21:00:00       []\n",
       "              2016-12-29 22:00:00       []\n",
       "              2016-12-29 23:00:00       []\n",
       "              2016-12-30 00:00:00       []\n",
       "              2016-12-30 01:00:00       []\n",
       "              2016-12-30 02:00:00       []\n",
       "              2016-12-30 03:00:00       []\n",
       "              2016-12-30 04:00:00       []\n",
       "              2016-12-30 05:00:00    False\n",
       "Name: polarity, dtype: object"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#DO AFTER\n",
    "##################################################################################\n",
    "# CALCULATE TIME MOOD CHANGES (DIURNAL VARIATION OF MOOD) LABELS: Early morning, mid morning, noon, afternoon, evening, etc.???\n",
    "# DESCRIPTION:\n",
    "# For each tweet, tokenize, group all scores for each mood where weighting is over 15%\n",
    "###################################################################################\n",
    "\n",
    "## FOCUS:\n",
    "## label tweet if posted between 12 and 4am\n",
    "# SAMPLE FORMAT:\n",
    "# user (Tom)\n",
    "# day (Jan 1 2017)\n",
    "# pos(per day- sentiment140) (13)\n",
    "# neg(per day- sentimen140)  (23)\n",
    "# angry (#of posts with high angry scores) (avg. score per day)\n",
    "# happy\n",
    "# sad \n",
    "# amused\n",
    "# ......\n",
    "# peaktime_d (peak time of continuous depressed posts of the day) (early morning)\n",
    "# peaktime_h (peak time of continuous happy posts of the day) (early morning)\n",
    "# Number tweets posted between 12 and 4am\n",
    "# hashtags_matched\n",
    "# ......\n",
    "\n",
    "#http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.resample.html\n",
    "#http://stackoverflow.com/questions/38575443/pandas-resample-to-return-just-one-column-after-an-apply-as-been-made\n",
    "def calcPeakTimes(args):\n",
    "    negative = 1\n",
    "    #for index, row in args.iterrows():\n",
    "       # if row['polarity'] == 0:\n",
    "    #negative += 1\n",
    "    #negative = 0\n",
    "    #if any(args['polarity'] == 2):\n",
    "        #args['negative'] = 1\n",
    "    #if args['polarity'].iloc[0]:\n",
    "    return negative#{'username' : args.username.values, 'negative' : negative}\n",
    "    #args.loc[:,'negative'] = negative\n",
    "    #search_neg = tweets_time_analysis.loc[tweets_time_analysis['polarity'] == 0]\n",
    "    \n",
    "\n",
    "\n",
    "tweets_time_analysis = bipolar_tweets_senti140_emotion_keywrds\n",
    "tweets_time_analysis = tweets_time_analysis.reset_index().set_index('tweetCreated')\n",
    "tweets_time_analysis.index = pd.to_datetime(tweets_time_analysis.index)\n",
    "\n",
    "#tweets_time_analysis = tweets_time_analysis.groupby(pd.TimeGrouper(freq='30m'))\n",
    "#df = tweets_time_analysis.resample(\"1d\").apply(calcPeakTimes)\n",
    "df = tweets_time_analysis.groupby('username').polarity.resample('1H').apply(lambda x: x == 0) #how='count'\n",
    "\n",
    "#tweets_time_analysis.polarity.value_counts()\n",
    "df #[['username', 'polarity', 'negative']]\n",
    "\n",
    "\n",
    "\n",
    "# TO DO TODAY\n",
    "# Sample 150 users with less than 50 perc url in tweets, and atleast 15% geotagged tweets\n",
    "# get most used geo-location\n",
    "# assign untagged tweets to that location\n",
    "# label tweets between 12 and 4am with most used geolocation\n",
    "\n",
    "\n",
    "\n",
    "## 2-5 continous 'low-mood' tweets within a 30 minute period\n",
    "## 2-5 continous 'happy-mood' tweets within a 30 minute period\n",
    "## 2-5 continous 'happy to low or low to happy' tweets within a 30 minute period (capture emotional unstableness)\n",
    "\n",
    "#for row in bipolar_tweets_senti140.itertuples():\n",
    "    #print row.username"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##################################################################################\n",
    "# REVERSE GEO-CODE LAT AND LON COORDINATES\n",
    "# DESCRIPTION\n",
    "# Use GeoNames web service to reverse geocode lat and lon coords, find points of interest\n",
    "# REFERENCES:\n",
    "# https://chrisalbon.com/python/pandas_missing_data.html\n",
    "# http://stackoverflow.com/questions/26701849/pandas-groupby-and-finding-maximum-in-groups-returning-value-and-count\n",
    "# http://stackoverflow.com/questions/6159074/given-the-lat-long-coordinates-how-can-we-find-out-the-city-country\n",
    "# http://www.geonames.org/maps/osm-reverse-geocoder.html\n",
    "# http://www.geonames.org/export/web-services.html#neighbourhood\n",
    "###################################################################################\n",
    "# http://www.shanelynn.ie/batch-geocoding-in-python-with-google-geocoding-api/\n",
    "# http://geocoder.readthedocs.io/api.html#reverse-geocoding\n",
    "# https://github.com/gregrobbins/geonames-python/blob/master/geonames.py\n",
    "# https://developers.google.com/chart/interactive/docs/gallery/geochart\n",
    "# http://stackoverflow.com/questions/26914900/reverse-geocoding-with-python-geocoder\n",
    "# http://geocode.xyz/api\n",
    "\n",
    "bipolar_groupby_user_tweets = bipolar_tweets_with_geo.groupby(['username','tweetLat','tweetLong'])['tweetLong'].agg({'count':'count'})\n",
    "bipolar_groupby_user_tweets = bipolar_groupby_user_tweets.reset_index()\n",
    "\n",
    "DOMAIN = 'http://api.geonames.org/'\n",
    "USERNAME = '' #enter your geonames username here\n",
    "\n",
    "def fetchJson(method, params):\n",
    "    uri = DOMAIN + '%s?%s&username=%s' % (method, urllib.urlencode(params), USERNAME)\n",
    "    resource = urllib2.urlopen(uri).readlines()\n",
    "    js = json.loads(resource[0])\n",
    "    return js\n",
    "def reverseGeocode2(row):\n",
    "    streets_nearby = []\n",
    "    lat_coords = row['tweetLat'] #37.451\n",
    "    long_coords = row['tweetLong'] #-122.18\n",
    "    \n",
    "\n",
    "    try:\n",
    "        \n",
    "        #resp = urllib2.urlopen(req)\n",
    "        #req.add_header('Content-Type', 'application/json')\n",
    "        \n",
    "        link = \"http://api.geonames.org/findNearbyStreetsOSMJSON?lat=%s&lng=%s&username=demo\" % (lat_coords, long_coords)\n",
    "        link = urllib.urlencode(link)\n",
    "        req = urllib2.Request(link)\n",
    "        response = urllib2.urlopen(req)\n",
    "        json_response = json.loads(response.read())\n",
    "\n",
    "        if json_response and \"status\" not in json_response and \"streetSegment\" in json_response:\n",
    "            for j in json_response['streetSegment']: #for key, value in data.iteritems():\n",
    "                if \"highway\" in j:\n",
    "                    streets_nearby.append(j['highway']) #j['streetSegment'].get('highway', 'None')\n",
    "                    \n",
    "            if len(streets_nearby) > 0:\n",
    "                print collections.Counter(streets_nearby).most_common()[0][0]\n",
    "                row['nearby'] = collections.Counter(streets_nearby).most_common()[0][0]\n",
    "            else:\n",
    "                row['nearby'] = 'None'\n",
    "                print 'None'\n",
    "        else:\n",
    "            # check if has status key and print message\n",
    "            if 'status' in json_response:\n",
    "                print json_response['status']['message']\n",
    "                row['nearby'] = json_response['status']['message']\n",
    "            else:\n",
    "                row['nearby'] = 'None'\n",
    "                print 'None'\n",
    "        return row\n",
    "        \n",
    "    except (urllib2.HTTPError, urllib2.URLError) as err:\n",
    "        print err\n",
    "        time.sleep(300)\n",
    "    except (Exception) as ex: \n",
    "        print ex\n",
    "        \n",
    "    \n",
    "#bipolar_groupby_user_tweets_df = splitDataFrameIntoSmaller(bipolar_groupby_user_tweets, 2000)\n",
    "#bipolar_groupby_user_tweets_df5 = bipolar_groupby_user_tweets_df[4].apply(reverseGeocode2, axis = 1)\n",
    "#bipolar_groupby_user_tweets_df5.to_csv('bipolar_groupby_user_tweets_df5.csv')\n",
    "#bipolar_groupby_user_tweets[bipolar_groupby_user_tweets['username']=='badkidvictoria']\n",
    "\n",
    "\n",
    "geolocator = Nominatim()\n",
    "location = geolocator.reverse(\"26.293909, 127.767219\")\n",
    "# can save bounding box using this service\n",
    "print location.address\n",
    "\n",
    "g = geocoder.google([26.293909, 127.767219], method='reverse')\n",
    "print g.address"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
